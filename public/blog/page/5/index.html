<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Blogs</title>
  <meta name="author" content="" />

  
  <meta name="keywords" content="devows, hugo, go">	
  

  
  <meta name="description" content="Site template made by devcows using hugo">	
  

  <meta name="generator" content="Hugo 0.36" />

  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="https://datafibers.orgcss/animate.css" rel="stylesheet">

  
  
    <link href="https://datafibers.orgcss/style.blue.css" rel="stylesheet" id="theme-stylesheet">
  


  
  <link href="https://datafibers.orgcss/custom.css" rel="stylesheet">

  
  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
  <link rel="shortcut icon" href="https://datafibers.orgimg/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="https://datafibers.orgimg/apple-touch-icon.png" />
  

  <link href="https://datafibers.orgcss/owl.carousel.css" rel="stylesheet">
  <link href="https://datafibers.orgcss/owl.theme.css" rel="stylesheet">

  <link rel="alternate" href="https://datafibers.org/index.xml" type="application/rss+xml" title="DataFibers">

  
  <meta property="og:title" content="Blogs" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="/blog//" />
  <meta property="og:image" content="img/logo.png" />

</head>


  <body>

    <div id="all">

        <header>

          <div class="navbar-affixed-top" data-spy="affix" data-offset-top="200">

    <div class="navbar navbar-default yamm" role="navigation" id="navbar">

        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="https://datafibers.org">
                    <img src="https://datafibers.orgimg/logo.png" alt="Blogs logo" class="hidden-xs hidden-sm">
                    <img src="https://datafibers.orgimg/logo-small.png" alt="Blogs logo" class="visible-xs visible-sm">
                    <span class="sr-only">Blogs - go to homepage</span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only">Toggle Navigation</span>
                        <i class="fa fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  <li class="dropdown">
                    
                    <a href="/">Home</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="https://github.com/datafibers-community/df_data_service/releases">Download</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/community/">Community</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/">Blog</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/training/">Training</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/contact/">Contact</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">

                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">

                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>

                </span>
                    </div>
                </form>

            </div>
            

        </div>
    </div>
    

</div>




        </header>

        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Blogs</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">
                <div class="row">
                    

                    <div class="col-md-9" id="blog-listing-medium">

                        
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2012-11-10-hadoop-job-patterns/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2012-11-10-hadoop-job-patterns/">Hadoop Job Patterns</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2012-11-10-hadoop-job-patterns/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">In order to handle complex hadoop jobs, such as DAG, you can use oozie. Here, I am talking about the native support of job handlers in the Hadoop MapReduce framework. The limitation is that it only support no cycle type of job flows
 With Job Config: You can also fire off multiple jobs in parallel by using Job.submit() instead of Job.wait ForCompletion(). The submit method returns immediately to the current thread and runs the job in the background.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2012-11-10-hadoop-job-patterns/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2013-05-09-hadoop-multiple-input-and-output/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2013-05-09-hadoop-multiple-input-and-output/">Hadoop Multiple Input and Output</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2013-05-09-hadoop-multiple-input-and-output/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">The following is an example of using multiple inputs (org.apache.hadoop.mapreduce.lib.input.MultipleInputs) with different input formats and different mapper implementations.
 MultipleInputs.addInputPath(job, accessLogPath, TextInputFormat.class, AccessLogMapper.class); MultipleInputs.addInputPath(job, userDataPath, TextInputFormat.class, UserDataMapper.class);  Note: static void addInputPath(Job job, Path path, Class&lt;? extends InputFormat&gt; inputFormatClass, Class&lt;? extends Mapper&gt; mapperClass): Add a Path with a custom InputFormat and Mapper to the list of inputs for the map-reduce job.
Similarly, there is MultipleOutputs to write multiple destinations
 MultipleOutputs.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2013-05-09-hadoop-multiple-input-and-output/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2012-07-06-hadoop-serialization-framework/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2012-07-06-hadoop-serialization-framework/">Hadoop Serialization Framework</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2012-07-06-hadoop-serialization-framework/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">Below are some mentioned in Hadoop In Practice and MapReduce Cookbooks
####Where to use serialization
 In order to be used as a value data type of a MapReduce computation, a data type must implement the org.apache.hadoop.io.Writable interface. The Writable interface which defines how Hadoop should serialize and de-serialize the values when transmitting and storing the data.
 In order to be used as a key data type of a MapReduce computation, a data type must implement the org.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2012-07-06-hadoop-serialization-framework/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2012-08-25-hadoop-split-and-block/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2012-08-25-hadoop-split-and-block/">Hadoop Split and Block</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2012-08-25-hadoop-split-and-block/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">There are two confusing concepts in Hadoop, block and split ####Block - A Physical Division HDFS was designed to hold and manage large amounts of data; a default block size is 64 MB. That means if a 128-MB text file was put in to HDFS, HDFS would divide the file into two blocks (128 MB/64 MB) and distribute the two chunks to the data nodes in the cluster. The main reason to setting proper block size is to balance the load of each map and reduce throughput so that the overall job takes better time to finish.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2012-08-25-hadoop-split-and-block/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2015-06-21-hadoop-streaming/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2015-06-21-hadoop-streaming/">Hadoop Streaming</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2015-06-21-hadoop-streaming/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">1. Streaming Overview Hadoop Streaming is a generic API which allows writing Mappers and Reduces in any language.
 Develop MapReduce jobs in practically any language Uses Unix Streams as communication mechanism between Hadoop and your code Any language that can read standard input and write are supported  Few good use-cases:
 Text processing - scripting languages do well in text analysis Utilities and/or expertise in languages other than Java  2.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2015-06-21-hadoop-streaming/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2012-12-23-hadoop-distcp-utility/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2012-12-23-hadoop-distcp-utility/">Hadoop distcp Utility</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2012-12-23-hadoop-distcp-utility/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">Hadoop distcp create a map only job to copy data across clusters Copy the weblogs folder from cluster A to cluster B:
hadoop distcp hdfs://namenodeA/data/weblogs hdfs://namenodeB/data/weblogs  Copy the weblogs folder from cluster A to cluster B, overwriting any existing files:
hadoop distcp –overwrite hdfs://namenodeA/data/weblogs \ hdfs://namenodeB/data/weblogs  Synchronize the weblogs folder between cluster A and cluster B:
hadoop distcp –update hdfs://namenodeA/data/weblogs \ hdfs://namenodeB/data/weblogs  Spefify number of mapper used</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2012-12-23-hadoop-distcp-utility/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2013-09-29-hive-composite-data-type/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2013-09-29-hive-composite-data-type/">Hive Composite Data Type</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2013-09-29-hive-composite-data-type/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">For now, hive supports following composite data type：
 map: (key1, value1, key2, value2, …). Creates a map with the given key/value pairs struct: (val1, val2, val3, …). Creates a struct with the given field values. Struct field names will be col1, col2, &hellip; named_struct: (name1, val1, name2, val2, …). Creates a struct with the given field names and values. (as of Hive 0.8.0) array: (val1, val2, …). Creates an array with the given elements create_union:(tag, val1, val2, …).</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2013-09-29-hive-composite-data-type/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2012-06-20-hive-performance-tuning/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2012-06-20-hive-performance-tuning/">Hive Performance Tuning - No. of MapReduce</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2012-06-20-hive-performance-tuning/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">###1. Set proper number of map
 通常情况下，作业会通过input的目录产生一个或者多个map任务。 主要的决定因素有： input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；
 举例： a) 假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b) 假设input目录下有3个文件a,b,c,大小分别为10m，20m，130m，那么hadoop会分隔成4个块（10m,20m,128m,2m）,从而产生4个map数 即，如果文件大于块大小(128m),那么会拆分，如果小于块大小，则把该文件当成一个块。
 是不是map数越多越好？
答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成， 而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。 而且，同时可执行的map数是受限的。
 是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录， 如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。
  针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数；
如何合并小文件，减少map数？
假设一个SQL任务：
Select count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’;  该任务的inputdir /group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04 共有194个文件，其中很多是远远小于128m的小文件，总大小9G，正常执行会用194个map任务。Map总共消耗的计算资源： SLOTS_MILLIS_MAPS= 623,020
我通过以下方法来在map执行前合并小文件，减少map数：
set mapred.max.split.size=100000000; set mapred.min.split.size.per.node=100000000; set mapred.min.split.size.per.rack=100000000; set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  再执行上面的语句，用了74个map任务，map消耗的计算资源：SLOTS_MILLIS_MAPS= 333,500 对于这个简单SQL任务，执行时间上可能差不多，但节省了一半的计算资源。
大概解释一下，100000000表示100M,
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并,最终生成了74个块。
如何适当的增加map数？
当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。假设有这样一个任务：
select data_desc,count(1),count(distinct id),sum(case when …), sum(case when .</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2012-06-20-hive-performance-tuning/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2012-06-18-hive-serde/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2012-06-18-hive-serde/">Hive SerDe</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2012-06-18-hive-serde/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">Unless there are no way to user internal parser, I do not recommend write user defined SerDe. Do not forget that Hive comes with a contrib RegexSerDeclass, which can tokenize your logs/files to resolve most problems:
CREATE EXTERNAL TABLE logs_20120101 ( host STRING, identity STRING, user STRING, time STRING, request STRING, status STRING, size STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( &quot;input.regex&quot; = &quot;([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\])([^ \&quot;]*|\&quot;[^\&quot;]*\&quot;) (-|[0-9]*) (-|[0-9]*)&quot;, &quot;output.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2012-06-18-hive-serde/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        <section class="post">
                            <div class="row">
                                <div class="col-md-4">
                                  <div class="image">
                                      <a href="https://datafibers.org/blog/1/01/01/2013-03-01-hive-sorting-and-ordering/">
                                          
                                          <img src="https://datafibers.orgimg/placeholder.png" class="img-responsive" alt="">
                                          
                                      </a>
                                  </div>
                                </div>
                                <div class="col-md-8">
                                    <h2><a href="https://datafibers.org/blog/1/01/01/2013-03-01-hive-sorting-and-ordering/">Hive Sorting and Ordering</a></h2>
                                    <div class="clearfix">
                                        <p class="author-category">
                                          
                                          

                                        </p>
                                        <p class="date-comments">
                                            <a href="https://datafibers.org/blog/1/01/01/2013-03-01-hive-sorting-and-ordering/"><i class="fa fa-calendar-o"></i> January 1, 0001</a>
                                        </p>
                                    </div>
                                    <p class="intro">There are following key words used in Hive to sort data with following difference:
 ORDER BY (ASC|DESC) : This is similar to the traditional SQL operator. Sorted order is maintained across all of the output from every reducer. It performs the global sort using only one reducer, so it takes long time to return result. The Usage with LIMIT is strongly recommended for order by. When hive.mapred.mode = strict and you do not specify “limit”, there are error out.</p>
                                    <p class="read-more"><a href="https://datafibers.org/blog/1/01/01/2013-03-01-hive-sorting-and-ordering/" class="btn btn-template-main">Continue reading</a>
                                    </p>
                                </div>
                            </div>
                        </section>
                        

                        <ul class="pager">
                            
                            <li class="previous"><a href="https://datafibers.org/blog/page/4/">&larr; Newer</a></li>
                            

                            
                            <li class="next"><a href="https://datafibers.org/blog/page/6/">Older &rarr;</a></li>
                            
                        </ul>
                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        

<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
      <h3 class="panel-title">Search</h3>
    </div>

    <div class="panel-body">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" role="search">
            <div class="input-group">
                <input type="search" name="q" class="form-control" placeholder="Search">
                <input type="hidden" name="sitesearch" value="https://datafibers.org">
                <span class="input-group-btn">
                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>
                </span>
            </div>
        </form>
    </div>
</div>







<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
      <h3 class="panel-title">Categories</h3>
    </div>

    <div class="panel-body">
        <ul class="nav nav-pills nav-stacked">
            
            <li><a href="https://datafibers.orgcategories/article">article (6)</a>
            </li>
            
            <li><a href="https://datafibers.orgcategories/programming">programming (2)</a>
            </li>
            
            <li><a href="https://datafibers.orgcategories/pseudo">pseudo (1)</a>
            </li>
            
            <li><a href="https://datafibers.orgcategories/release">release (3)</a>
            </li>
            
            <li><a href="https://datafibers.orgcategories/review">review (1)</a>
            </li>
            
            <li><a href="https://datafibers.orgcategories/tutorial">tutorial (1)</a>
            </li>
            
        </ul>
    </div>
</div>








<div class="panel sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title">Tags</h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            <li><a href="https://datafibers.orgtags/bigdata"><i class="fa fa-tags"></i> bigdata</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/datafibers"><i class="fa fa-tags"></i> datafibers</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/datamining"><i class="fa fa-tags"></i> datamining</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/git"><i class="fa fa-tags"></i> git</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/go"><i class="fa fa-tags"></i> go</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/hadoop"><i class="fa fa-tags"></i> hadoop</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/hbase"><i class="fa fa-tags"></i> hbase</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/hive"><i class="fa fa-tags"></i> hive</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/hugo"><i class="fa fa-tags"></i> hugo</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/kafka"><i class="fa fa-tags"></i> kafka</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/maven"><i class="fa fa-tags"></i> maven</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/pig"><i class="fa fa-tags"></i> pig</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/programming"><i class="fa fa-tags"></i> programming</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/scala"><i class="fa fa-tags"></i> scala</a>
            </li>
            
            <li><a href="https://datafibers.orgtags/spark"><i class="fa fa-tags"></i> spark</a>
            </li>
            
        </ul>
    </div>
</div>






                        

                    </div>
                    

                    

                </div>
                
            </div>
            
        </div>
        

        <footer id="footer">
    <div class="container">

        
        <div class="col-md-4 col-sm-6">
            <h4>About us</h4>

            DataFibers are expertises by applying its cutting edge big data technologies and solutions on enterprise big data centric use cases.

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

        <div class="col-md-4 col-sm-6">

             
            <h4>Recent posts</h4>

            <div class="blog-entries">
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://datafibers.org/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/">
                          
                            <img src="https://datafibers.org/img/banners/maxmin.jpg" class="img-responsive" alt="Hive Get the Max/Min">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://datafibers.org/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/">Hive Get the Max/Min</a></h5>
                    </div>
                </div>
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://datafibers.org/blog/2018/01/10/2018-01-10-reviews-big-data/">
                          
                            <img src="https://datafibers.org/img/banners/book_reviews.jpg" class="img-responsive" alt="Big Data Books Reviews">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://datafibers.org/blog/2018/01/10/2018-01-10-reviews-big-data/">Big Data Books Reviews</a></h5>
                    </div>
                </div>
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://datafibers.org/blog/2017/12/22/2017-12-22-df-winter-release/">
                          
                            <img src="https://datafibers.org/img/banners/df_release.jpg" class="img-responsive" alt="2017 Winter Release">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://datafibers.org/blog/2017/12/22/2017-12-22-df-winter-release/">2017 Winter Release</a></h5>
                    </div>
                </div>
                
            </div>

            <hr class="hidden-md hidden-lg">
             

        </div>
        

        
        <div class="col-md-4 col-sm-6">

          <h4>Contact</h4>

            <strong>DataFibers.</strong>
        <br>7030 Woodbine Avenue,
        <br>L3R 6G2
        <br>Ontario, Markham
        <br>
        <strong>Canada</strong>
      </p>
      


            <a href="/contact" class="btn btn-small btn-template-main">Go to contact page</a>

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-12">
            
            <p class="pull-left">Copyright (c) 2018, DataFibers all rights reserved.</p>
            
            <p class="pull-right">
              Template by <a href="http://bootstrapious.com/free-templates">Bootstrapious</a>.
              

              Powered by <a href="https://gohugo.io/">Hugo</a>
            </p>
        </div>
    </div>
</div>





    </div>
    

    
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-40213017-3', 'auto');
ga('send', 'pageview');
</script>

<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="//maps.googleapis.com/maps/api/js?key=AIzaSyCksU3IZTuUK6fA-doQUfQ4KcYm7mB_vlk&v=3.exp"></script>

<script src="https://datafibers.orgjs/hpneo.gmaps.js"></script>
<script src="https://datafibers.orgjs/gmaps.init.js"></script>
<script src="https://datafibers.orgjs/front.js"></script>


<script src="https://datafibers.orgjs/owl.carousel.min.js"></script>


  </body>
</html>
