<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hive on DataFibers</title>
    <link>https://datafibers.org/tags/hive/</link>
    <description>Recent content in Hive on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Feb 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers.org/tags/hive/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers.org/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
&amp;gt; SELECT name,sex_age.sex AS sex,sex_age.age AS age FROM employee; +----------+---------+------+ | name | sex | age | +----------+---------+------+ | Michael | Male | 30 | | Will | Male | 35 | | Shelley | Female | 27 | | Lucy | Female | 57 | | Steven | Male | 30 | +----------+---------+------+ 5 rows selected (75.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers.org/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers.org/blog/2017/11/01/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Wed, 01 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2017/11/01/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
    <item>
      <title>Apache Hive Essentials Published</title>
      <link>https://datafibers.org/blog/1/01/01/2015-03-15-apache-hive-essentials-published/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers.org/blog/1/01/01/2015-03-15-apache-hive-essentials-published/</guid>
      <description>Finally, I made it. I got it published after working for 6 monthes.
Apache Hive Essentials  My very first book Also the first book on Apache Hive 1.0.0 in the world  Check it out here</description>
    </item>
    
    <item>
      <title>Hive Composite Data Type</title>
      <link>https://datafibers.org/blog/1/01/01/2013-09-29-hive-composite-data-type/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers.org/blog/1/01/01/2013-09-29-hive-composite-data-type/</guid>
      <description>For now, hive supports following composite data type：
 map: (key1, value1, key2, value2, …). Creates a map with the given key/value pairs struct: (val1, val2, val3, …). Creates a struct with the given field values. Struct field names will be col1, col2, &amp;hellip; named_struct: (name1, val1, name2, val2, …). Creates a struct with the given field names and values. (as of Hive 0.8.0) array: (val1, val2, …). Creates an array with the given elements create_union:(tag, val1, val2, …).</description>
    </item>
    
    <item>
      <title>Hive Performance Tuning - No. of MapReduce</title>
      <link>https://datafibers.org/blog/1/01/01/2012-06-20-hive-performance-tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers.org/blog/1/01/01/2012-06-20-hive-performance-tuning/</guid>
      <description>###1. Set proper number of map
 通常情况下，作业会通过input的目录产生一个或者多个map任务。 主要的决定因素有： input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；
 举例： a) 假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b) 假设input目录下有3个文件a,b,c,大小分别为10m，20m，130m，那么hadoop会分隔成4个块（10m,20m,128m,2m）,从而产生4个map数 即，如果文件大于块大小(128m),那么会拆分，如果小于块大小，则把该文件当成一个块。
 是不是map数越多越好？
答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成， 而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。 而且，同时可执行的map数是受限的。
 是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录， 如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。
  针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数；
如何合并小文件，减少map数？
假设一个SQL任务：
Select count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’;  该任务的inputdir /group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04 共有194个文件，其中很多是远远小于128m的小文件，总大小9G，正常执行会用194个map任务。Map总共消耗的计算资源： SLOTS_MILLIS_MAPS= 623,020
我通过以下方法来在map执行前合并小文件，减少map数：
set mapred.max.split.size=100000000; set mapred.min.split.size.per.node=100000000; set mapred.min.split.size.per.rack=100000000; set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  再执行上面的语句，用了74个map任务，map消耗的计算资源：SLOTS_MILLIS_MAPS= 333,500 对于这个简单SQL任务，执行时间上可能差不多，但节省了一半的计算资源。
大概解释一下，100000000表示100M,
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并,最终生成了74个块。
如何适当的增加map数？
当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。假设有这样一个任务：
select data_desc,count(1),count(distinct id),sum(case when …), sum(case when .</description>
    </item>
    
    <item>
      <title>Hive and Hadoop Exceptions</title>
      <link>https://datafibers.org/blog/1/01/01/2014-06-01-hive-and-hadoop-exceptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers.org/blog/1/01/01/2014-06-01-hive-and-hadoop-exceptions/</guid>
      <description>I installed Hive 1.0.0 on Hadoop 1.2.1. When I try to enter the Hive CLI, it reports following exceptions
org.apache.hadoop.hive.ql.metadata.HiveException:java.io.IOException:Filesystem closed  According to the search here, the mainly reason for this is that when multiple nodes read HFDS files if one node is offline, it will throw such exception when the other nodes are still reading the data cached. There are two ways to resolve this.
 Turn off JVM reuse  &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapred.</description>
    </item>
    
    <item>
      <title>Hive and Hadoop Exceptions</title>
      <link>https://datafibers.org/blog/1/01/01/2015-02-12-hive-and-hadoop-exceptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers.org/blog/1/01/01/2015-02-12-hive-and-hadoop-exceptions/</guid>
      <description>I installed Hive 1.0.0 on Hadoop 1.2.1. When I try to enter the Hive CLI, it reports following exceptions
org.apache.hadoop.hive.ql.metadata.HiveException:java.io.IOException:Filesystem closed  According to the search here, the mainly reason for this is that when multiple nodes read HFDS files if one node is offline, it will throw such exception when the other nodes are still reading the data cached. There are two ways to resolve this.
 Turn off JVM reuse  &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapred.</description>
    </item>
    
    <item>
      <title>Hive vs. Pig</title>
      <link>https://datafibers.org/blog/1/01/01/2013-04-24-hive-vs-pig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers.org/blog/1/01/01/2013-04-24-hive-vs-pig/</guid>
      <description>Both projects are top Apache projects to process data in Hadoop. Here, I try to compare the difference.
Below is picture I found (I cannot find the original link, but there is mirror here)
In addition, I am here to share some more difference. To be honest, I prefer pig more. Mapred, Hive, and pig are like Java, ruby, and python.
Pig
 It is more easy to install pig than hive.</description>
    </item>
    
  </channel>
</rss>