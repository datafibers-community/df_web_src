<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Article on DataFibers</title>
    <link>https://datafibers.org/categories/article/</link>
    <description>Recent content in Article on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Feb 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers.org/categories/article/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers.org/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
&amp;gt; SELECT name,sex_age.sex AS sex,sex_age.age AS age FROM employee; +----------+---------+------+ | name | sex | age | +----------+---------+------+ | Michael | Male | 30 | | Will | Male | 35 | | Shelley | Female | 27 | | Lucy | Female | 57 | | Steven | Male | 30 | +----------+---------+------+ 5 rows selected (75.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers.org/blog/2017/11/01/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Wed, 01 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2017/11/01/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
    <item>
      <title>Apache Kafka Overview</title>
      <link>https://datafibers.org/blog/2017/10/05/2017-10-05-apache-kafka-overview/</link>
      <pubDate>Thu, 05 Oct 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2017/10/05/2017-10-05-apache-kafka-overview/</guid>
      <description>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, and Pig were all designed to process data in batches rather than continuously. Recently enterprises have discovered the power of analyzing and processing data and events as they happen instead of batches. Most traditional messaging systems, such as RabbitMq, neither scale up to handle big data in realtime nor use friendly with big data ecosystem.</description>
    </item>
    
    <item>
      <title>Scala Apply Method</title>
      <link>https://datafibers.org/blog/2017/09/15/2017-09-15-scala-apply-method/</link>
      <pubDate>Fri, 15 Sep 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2017/09/15/2017-09-15-scala-apply-method/</guid>
      <description>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics like java array access for an arbitrary class.
For example, we create a class of RiceCooker and its method cook to cook rice. Whenever we need to cook rice, we could call this method.
class RiceCooker { def cook(cup_of_rice: Rice) = { cup_of_rice.isDone = true cup_of_rice } } val my_rice_cooker: RiceCooker = new RiceCooker() my_rice_cooker.</description>
    </item>
    
    <item>
      <title>One Platform Initatives for Spark</title>
      <link>https://datafibers.org/blog/2017/06/24/2015-09-23-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</link>
      <pubDate>Sat, 24 Jun 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2017/06/24/2015-09-23-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</guid>
      <description>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.
The Spark is originally invented by few guys who started up the Databrick. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn.</description>
    </item>
    
    <item>
      <title>Migrate from Jekyll</title>
      <link>https://datafibers.org/blog/2015/10/10/migrate-from-jekyll/</link>
      <pubDate>Sat, 10 Oct 2015 13:07:31 +0200</pubDate>
      
      <guid>https://datafibers.org/blog/2015/10/10/migrate-from-jekyll/</guid>
      <description>Move static content to static Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like
▾ &amp;lt;root&amp;gt;/ ▾ images/ logo.png  should become
▾ &amp;lt;root&amp;gt;/ ▾ static/ ▾ images/ logo.png  Additionally, you&amp;rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.</description>
    </item>
    
  </channel>
</rss>